{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transformers: Attention Is All You Need\"\n",
    "author: \"richsi\"\n",
    "categories: [Attention, Paper, Transformer]\n",
    "date: \"2024-12-30\"\n",
    "image: \"images/thumbnail.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Ashish Vaswani et al., 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Importing packages\n",
    "#| code-fold: true\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* What is the topic of this guide, and why is it important?\n",
    "\n",
    "The transformer is an attention-based network architecture that learns context and meaning by tracking relationships in sequential data like words in a sentence. \n",
    "\n",
    "* What problem does this concept or paper aim to solve? \n",
    "\n",
    "Sequence modeling and transduction problems such as machine translation have previously been solved by recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). However, due to sequential dependency, the inherent nature of RNNs prevents its training from being parallelized. \n",
    "\n",
    "* What are the main contributions or breakthroughs introduced?\n",
    "\n",
    "Transformers show significant improvements in both computational efficiency as well as model performance.\n",
    "\n",
    "* How does this fit into the broader context of the field?\n",
    "\n",
    "This paper laid the groundwork for state-of-the-art models such as BERT and GPT which has revolutionized the natural language processing (NLP) field. Beyong NLP, transformers have successfully adapted to other machine learning domains like computer vision and reinforcement learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled Dot-Product Attention takes in three inputs: query $\\bf{Q}$, key $\\bf{K}$, and value $\\bf{V}$, where $X$ is the input and $W^Q$, $W^K$, $W^V$ are learnable weight matrices specific to queries, keys, and values. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q = XW^Q\\\\\n",
    "K = XW^K\\\\\n",
    "V = XW^V\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\bf{X}$ represents the input embeddings to the transformer layer. For the first layer, $X$ = Word Embedding + Positional Encoding. Word embedding is a vector that represents the semantic meaning of a word (or token).  Positional encoding is a vector that contains informations about a token's position in a sequence by mapping that information to a latent space.\n",
    "\n",
    "* $\\bf{Q}$ represents the current token's representation to \"query\" information from other tokens. It's derived from the combined word and positional information.\n",
    "\n",
    "* $\\bf{K}$ represents all tokens in the space and helps determine the relevance of each token to the query token. Like $Q$, it's based on the same input embeddings (word + positional) but transformed different via $W^K$.\n",
    "\n",
    "* $\\bf{V}$ represents the information associated with each token. It is the actual data that gets weighted and aggregated based on attention scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of $Q$, $K$, and $V$ are determined by the model's **latent space dimensionality** ($d_{model}$) and the number of **attention heads** ($h$). In this example, we initialize the latent space dimension to be **512** and the number of attention heads to be **8**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "  \"\"\" \n",
    "  Compute scaled dot-product attention\n",
    "\n",
    "  Args:\n",
    "  Q: Query matrix, shape (N, d_k)\n",
    "  K: Key matrix, shape (N, d_k)\n",
    "  V: Value matrix, shape (N, d_v)\n",
    "\n",
    "  Returns:\n",
    "  output: Weighted sum of values after applying attention, shape (N, d_v)\n",
    "  attention_weights: shape (N, N)\n",
    "  \"\"\"\n",
    "  d_k = Q.shape[1] # Dimensionality of keys/queries\n",
    "  # Computing scaled attention scores\n",
    "  scores = np.dot(Q, K.T) / np.sqrt(d_k) # Shape: (N,d_v) @ (d_v,N) -> (N, N)\n",
    "  \n",
    "  # Apply softmax to get attention weights\n",
    "  attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1) # axis=1 for column wise addition\n",
    "\n",
    "  # Computing weighted sum of values\n",
    "  output = np.dot(attention_weights, V) # Shape: (N,N) @ (N,d_v) -> (N,d_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications and Insights\n",
    "\n",
    "* Where and how is this concept applied in the real world?\n",
    "* What problems does it solve, and what value does it provide?\n",
    "* What are some examples or use cases where this has been impactful?\n",
    "* What are the broader implications or insights gained from this work?\n",
    "* How does this contribute to advancements in the field or industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* What are the key takeaways or lessons from this guide?\n",
    "* Why is this concept significant in the broader context of the field?\n",
    "* What questions remain unanswered or open for further exploration?\n",
    "* What resources or next steps can help deepen understanding?\n",
    "* How can this knowledge be applied or expanded upon in practice?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
