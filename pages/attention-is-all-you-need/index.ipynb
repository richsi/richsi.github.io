{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transformers: Attention Is All You Need\"\n",
    "author: \"richsi\"\n",
    "categories: [Attention, Paper, Transformer]\n",
    "date: \"2024-12-30\"\n",
    "image: \"images/thumbnail.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) by Ashish Vaswani et al., 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* What is the topic of this guide, and why is it important?\n",
    "\n",
    "The transformer is an attention-based network architecture that learns context and meaning by tracking relationships in sequential data like words in a sentence. \n",
    "\n",
    "* What problem does this concept or paper aim to solve? \n",
    "\n",
    "Sequence modeling and transduction problems such as machine translation have previously been solved by recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). However, due to sequential dependency, the inherent nature of RNNs prevents its training from being parallelized. \n",
    "\n",
    "* What are the main contributions or breakthroughs introduced?\n",
    "\n",
    "Transformers show significant improvements in both computational efficiency as well as model performance.\n",
    "\n",
    "* How does this fit into the broader context of the field?\n",
    "\n",
    "This paper laid the groundwork for state-of-the-art models such as BERT and GPT which has revolutionized the natural language processing (NLP) field. Beyong NLP, transformers have successfully adapted to other machine learning domains like computer vision and reinforcement learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications and Insights\n",
    "\n",
    "* Where and how is this concept applied in the real world?\n",
    "* What problems does it solve, and what value does it provide?\n",
    "* What are some examples or use cases where this has been impactful?\n",
    "* What are the broader implications or insights gained from this work?\n",
    "* How does this contribute to advancements in the field or industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* What are the key takeaways or lessons from this guide?\n",
    "* Why is this concept significant in the broader context of the field?\n",
    "* What questions remain unanswered or open for further exploration?\n",
    "* What resources or next steps can help deepen understanding?\n",
    "* How can this knowledge be applied or expanded upon in practice?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
