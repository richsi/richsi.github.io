[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Richard’s Page",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTransformers: Attention Is All You Need\n\n\n\nAttention\n\n\nPaper\n\n\nTransformer\n\n\n\n\n\n\n\nrichsi\n\n\nDec 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning - Tom Mitchell\n\n\n\nBook\n\n\nNotes\n\n\n\n\n\n\n\nrichsi\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I’m Richard"
  },
  {
    "objectID": "pages/machine-learning-tom-mitchell/index.html",
    "href": "pages/machine-learning-tom-mitchell/index.html",
    "title": "Machine Learning - Tom Mitchell",
    "section": "",
    "text": "Introduction\nMachine learning is inherently a multidisciplinary field. It draws on results from artificial intelligence, probability, statistics, complexity theory, and many more.\n\n\nWell-Posed Learning Problems\n\nDefinition: A computer program is said to learn from experience \\(E\\) with respect to some class of tasks \\(T\\) and performance measure \\(P\\), if its performance at tasks in \\(T\\), as measured by \\(P\\), improves with experience \\(E\\).\n\nRobot Driving Learning Problem:\n\nTask \\(T\\): driving on public four-lane highways using vision sensors\nPerformance measure \\(P\\): average distance traveled before an error\nTraining experience \\(E\\): a sequence of images and steering commands recorded while observing a human driver\n\n\n\n\nDesigning a Learning System\nConsider designing a program to learn to player checkers. Assume no external teacher.\n\nTask \\(T\\): playing checkers\nPerformance measure \\(P\\): percent of games won\nTraining experience \\(E\\): games played against itself\n\nChoosing the Training Experience\n\nOne key attribute is whether the training experience provides direct or indirect feedback regarding the choices made by the performance system\n\nDirect training examples can consist of individual checker board states and the correct move for each\nIndirect training examples may consist of move sequences and their final outcomes\n\nA second important attribute of the training experience is the degree to which the learner controls the sequence of training examples\nA third important attribute is how well it represents the distribution of examples over which the final system performance \\(P\\) must be measured\n\nIn practice, it is often necessary to learn from a distribution of examples that is somewhat different from those on which the final system will be evaluated\nMost current theory of machine learning rests on the assumption that the distribution of training examples is identical to the distribution of test examples\n\n\nChoosing the Target Function\nThe target function will determine what type of knowledge will be learned and how it will be used by the performance program. Let us assume a target function \\(V\\) that maps any legal board state from the set \\(B\\) to some real value. We intend for this target function \\(V\\) to assign higher scores to better board states\nLet us define the target value \\(V(b)\\) for an arbitrary board state \\(b\\) in \\(B\\), as follows:\n\nif \\(b\\) is a final board state that is won, then \\(V(b)\\) = 100\nif \\(b\\) is a final board state that is lost, then \\(V(b)\\) = -100\nif \\(b\\) is a final board state that is drawn, then \\(V(b)\\) = 0\nif \\(b\\) is not a final state in the game, then \\(V(b) = V(b')\\) where \\(b'\\) is the best final board state that can be achieved starting from \\(b\\) and playing optimally until the end of the game\n\nHowever, searching for the optimal line of play, all the way until the end of the game, is not efficiently computable. The goal of learning in this case is to discover an operational description of \\(V\\). A description that can be used by the checkers-playing program to evaluate states and select moves within realistic time bounds. Thus, we have reduced the learning task in this case to the problem of discovering an operational description of the ideal target function \\(V\\).\nChoosing a Representation for the Target Function\nAfter specifying the ideal target function \\(V\\), we must choose a representation that the learning program will use to describe the function \\(\\hat{V}\\) that it will learn. The choice of representation involves a crucial tradeoff. On one hand, we wish to pick a very expressive representation to allow represnting as close an approximation as possible to the target function \\(V\\). On the other hand, the more expressive the representation, the more training data the program will require in order to choose among the alternative hypotheses it can represent.\nChoosing a Function Approximation Algorithm\nWhile it is easy to assign a value to board states that correspond to the end of the game, it is less obvious how to assign training values to the intermediate board states that occur before the game’s end. The rule for estimating training values is summarized as\n\nRule for estimating training values: \\[\nV_{train}(b) \\leftarrow \\hat{V}(Successor(b))\n\\]\n\n\\(\\hat{V}\\) denotes the learner’s current approximation to \\(V\\) and \\(Successor(b)\\) denotes the next board state following \\(b\\). Iteratively estimating training values based on estimates of successor state values can be proven to converge toward perfect estimates of \\(V_{train}\\).\nAdjusting the Weights\nFirst, define the best fit to the training data. One common approach is to define the best hypothesis, or set of weights, which minimizes the squared error \\(E\\) between training (real) values and values predicted by hypothesis \\(\\hat{V}\\).\n\n\\[\nE \\equiv \\sum_{\\langle b, V_{train}(b) \\rangle \\in\\ training\\ examples} (V_{train}(b) - \\hat{V}(b))^2\n\\]\n\nThus, we seek the weights, or equivalently the \\(\\hat{V}\\), that minimizes \\(E\\) for the observed training examples. Many algorithms will incremently refine the weights as new training examples become avavilable and become rebust to errors in these estimated training values. One example is least mean squares (LMS), which performs stochastic gradient-descent search through the space of hypotheses (weight values) to minimize the squared error \\(E\\).\n\nLMS weight update rule\nFor each training example \\(\\langle b, V_{train}(b)\\rangle\\)\n\nUse the current weights to calculate \\(\\hat{V}(b)\\)\nFor each weight \\(w_i\\), update it as \\(w_i \\leftarrow w_i + \\eta (V_{train}(b) - \\hat{V}(b))x_i\\)\n\n\nHere, \\(\\eta\\) is a small constant (e.g., 0.1) that moderates the size of the weight update. For a deeper intuitive understanding, notice that when the error \\((V_{train}(b) - \\hat{V}(b))\\) is zero, the weights are not changed. However, if the is positive, that means \\(\\hat{V}(b)\\) is too small and the weight is increased in proportion to the value of its corresponding feature.\n\n\n\n\nDecision Tree Learning\nDecision tree (DT) learning is one of the most commonly used methods for inductive inference. It is a method for approximating discrete-valued target functions that is robust to noisy data and capable of learning disjunctive expressions.\n\nDecision Tree Representation\nDTs classify instances by sorting a tree from the root to some leaf node, which provides classification of the instance. Each node in the tree specifies some attribute of the instance. Each path from the tree root to a leaf corresponds to a conjunction of attribute tests\n\n\n\nAppropriate Problems for Decision Tree Learning\nDT Learning is generally best suited to problems with the following characteristics:\n\nInstances are represented by attribute-value pairs. Instances are described by a fixed set of attributes (e.g., Temperature) and their values (e.g., Hot or 72F)\nThe target function has discrete output values\nDisjunctive descriptions may be required. DT naturally represent disjunctive expressions\nThe training data may contain errors. DT learning methods are robust to errors, both errors in classifications of the training examples and errors in the attribute values that describe these examples\nThe training data may contain missing attribute values. DT methods can be used even when some training examples have unknown values\n\n\n\n\nThe Basic Decision Tree Learning Algorithm\nMost algorithms that have been developed for learning DT trees are variations on a core algorithm that employs a top-down, greedy search through the space of possible DTs. The algorithms we will look at is ID3 (Quinlan 1986).\nHow Does ID3 Work?\n\nLearns DTs by constructing them top-down by evaluating each instance attribute to determine how well it classifies the training examples\nThe best attribute is selected and used as the test at the root node of the tree\nA descendant of the root node is then created for each possible value of this attribute, and the training examples are sorted to the appropriate descendant node\n\nThis forms a greedy search for an acceptable DT, in which the algorithm never backtracks to reconsider earlier choices.\nWhich Attribute Is the Best Classifier?\nWe would like to select the attribute that is most useful for classifying examples. We define a statistical property, called information gain, as a measure for how well a given attribute separates the training examples according to their target classification.\nTo precisely define information gain, we use entropy. Entropy chracterizes the (im)purity of an arbitrary collection of examples. Given a collection \\(S\\), containing positive and negative examples, the entropy of \\(S\\) relative to boolean classification is\n\\[\nEntropy(S) \\equiv -p_+ log_2 p_+ - p_- log_2 p_-\n\\]\nwhere \\(p_+\\) is the proportion of positive examples in \\(S\\) and \\(p_-\\) is the proportion of negative examples in \\(S\\). We define 0 log 0 to be 0 in all calculations involving entropy. For example,\n\\[\nEntropy([9+, 5-]) = -(9/14)log_2 (9/14) - (5/14)log_2 (5/14) = 0.940\n\\]\nNotice entropy is 0 if all members of \\(S\\) belongs to the same class. Entropy will be 1 when the collection contains an equal number of positive and negative samples.\n\n\n\n\nEntropy of \\(S\\) relative to \\(c\\) different target attributes is defined as\n\n\\[\nEntropy(S) \\equiv \\sum_{i=1}^{c} -p_i log_2 p_i\n\\]\n\nInformation gain measures the expected reduction in entropy. The information gain, \\(Gain(S, A)\\) of an attribute \\(A\\), relative to a collection of examples \\(S\\), is defined as\n\n\\[\nGain(S, A) \\equiv Entropy(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n\\]\n\nFor example, suppose \\(S\\) is a collection of days described by the attributes including \\(Wind\\), which can have the values \\(Weak\\) or \\(Strong\\). The information gain due to sorting the original 14 examples by the attribute \\(Wind\\) may be calculated as\n\n\\[\n\\begin{aligned}\nValues(Wind) &= Weak, Strong\\\\\nS &= [9+, 5-] \\\\\nS_{Weak} &\\leftarrow [6+, 2-]\\\\\nS_{Strong} &\\leftarrow [3+, 3-]\\\\\nGain(S, Wind) &= Entropy(S) - \\sum_{v\\in(Weak, Strong)} \\frac{|S_v|}{|S|} Entropy(S_v) \\\\\n&= Entropy(S) - (8/14)Entropy(S_{Weak})\\\\\n&\\quad- (6/14)Entropy(S_{Strong})\\\\\n&= 0.940 - (8/14)0.811 - (6/14)1.00\\\\\n&=0.048\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html",
    "href": "pages/attention-is-all-you-need/index.html",
    "title": "Transformers: Attention Is All You Need",
    "section": "",
    "text": "“Attention Is All You Need” by Ashish Vaswani et al., 2017.\nImporting packages\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#introduction",
    "href": "pages/attention-is-all-you-need/index.html#introduction",
    "title": "Transformers: Attention Is All You Need",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the topic of this guide, and why is it important?\n\nThe transformer is an attention-based network architecture that learns context and meaning by tracking relationships in sequential data like words in a sentence.\n\nWhat problem does this concept or paper aim to solve?\n\nSequence modeling and transduction problems such as machine translation have previously been solved by recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). However, due to sequential dependency, the inherent nature of RNNs prevents its training from being parallelized.\n\nWhat are the main contributions or breakthroughs introduced?\n\nTransformers show significant improvements in both computational efficiency as well as model performance.\n\nHow does this fit into the broader context of the field?\n\nThis paper laid the groundwork for state-of-the-art models such as BERT and GPT which has revolutionized the natural language processing (NLP) field. Beyong NLP, transformers have successfully adapted to other machine learning domains like computer vision and reinforcement learning."
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#methods",
    "href": "pages/attention-is-all-you-need/index.html#methods",
    "title": "Transformers: Attention Is All You Need",
    "section": "Methods",
    "text": "Methods\n\nEncoder Decoder Stacks\nEncoder:\nDecoder:\n\n\nScaled Dot-Product Attention\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n\\]\nScaled Dot-Product Attention takes in three inputs: query \\(\\bf{Q}\\), key \\(\\bf{K}\\), and value \\(\\bf{V}\\), where \\(X\\) is the input and \\(W^Q\\), \\(W^K\\), \\(W^V\\) are learnable weight matrices specific to queries, keys, and values.\n\\[\n\\begin{aligned}\nQ = XW^Q\\\\\nK = XW^K\\\\\nV = XW^V\\\\\n\\end{aligned}\n\\]\n\n\\(\\bf{X}\\) represents the input embeddings to the transformer layer. For the first layer, \\(X\\) = Word Embedding + Positional Encoding. In subsequent layers, \\(X\\) is the output of the previous transformer layer. Word embedding is a vector that represents the semantic meaning of a word (or token). Positional encoding is a vector that contains informations about a token’s position in a sequence by mapping that information to a latent space.\n\\(\\bf{Q}\\) represents the current token’s representation to “query” information from other tokens. It’s derived from the combined word and positional information.\n\\(\\bf{K}\\) represents all tokens in the space and helps determine the relevance of each token to the query token. Like \\(Q\\), it’s based on the same input embeddings (word + positional) but transformed different via \\(W^K\\).\n\\(\\bf{V}\\) represents the information associated with each token. It is the actual data that gets weighted and aggregated based on attention scores.\n\nThe dimensions of \\(Q\\), \\(K\\), and \\(V\\) are determined by the model’s latent space dimensionality (\\(d_{model}\\)) and the number of attention heads (\\(h\\)). In this example, we initialize the latent space dimension to be 512 and the number of attention heads to be 8.\n\ndef scaled_dot_product_attention(Q, K, V):\n  \"\"\" \n  Compute scaled dot-product attention\n\n  Parameters:\n  Q, K, V: Query, Key, and Value matrices (numpy arrays).\n      Shapes: (N, d_k) for Q and K, (N, d_v) for V.\n\n  Returns:\n  output: Weighted sum of values after applying attention, shape (N, d_v)\n  attention_weights: shape (N, N)\n  \"\"\"\n  d_k = Q.shape[1] # Dimensionality of keys/queries\n  scores = np.dot(Q, K.T) / np.sqrt(d_k) # Shape: (N,d_v) @ (d_v,N) -&gt; (N, N)\n  attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1) # axis=1 for column wise addition\n  output = np.dot(attention_weights, V) # Shape: (N,N) @ (N,d_v) -&gt; (N,d_v)\n\n  return output, attention_weights\n\n\n\nMulti-Head Attention\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Attention}_1, \\text{Attention}_2, ..., \\text{Attention}_h)W^O\n\\]\nMulti-head attention scales the concept of scaled dot-product attention by running multiple attention heads in parallel. Each haed operates in a different subspace which optimizes feature learning. For example, one head might focus on syntax while another head focuses on semantics.\nSteps:\n\nLinear Projections: Split \\(Q, K, V\\) into \\(h\\) subspaces, one for each head.\nParallel Attention: Perform scaled dot-product attention independently for each head.\nConcatenation: Combine the outputs of all heads into a single matrix.\nFinal Linear Transformation: Project the concatenated matrix back to \\(d_{model}\\) using \\(W^O\\).\n\n\ndef multi_head_attention(Q, K, V, num_heads, d_model):\n  \"\"\"\n  Computes multi-head attention.\n\n  Parameters:\n  Q, K, V: Query, Key, and Value matrices (numpy arrays).\n      Shapes: (N, d_k) for Q and K, (N, d_v) for V. \n  num_heads: Number of attention heads.\n  d_model: Dimensionality of the model.\n\n  Returns:\n  output: Multi-head attention output. Shape: (N, d_model)\n  \"\"\"\n  d_k = d_model // num_heads\n  d_v = d_k # same as d_k\n\n  # Initializing weights\n  W_Q = np.random.rand(num_heads, d_model, d_k)\n  W_K = np.random.rand(num_heads, d_model, d_k)\n  W_V = np.random.rand(num_heads, d_model, d_v)\n  W_O = np.random.rand(num_heads * d_v, d_model)\n\n  # Split heads and compute attention\n  attention_heads = []\n  for i in range(num_heads):\n    Q_i = np.dot(Q, W_Q[i]) # Shape: (N, d_k)\n    K_i = np.dot(Q, W_K[i]) # Shape: (N, d_k)\n    V_i = np.dot(Q, W_V[i]) # Shape: (N, d_v)\n    head_output, _ = scaled_dot_product_attention(Q_i, K_i, V_i)\n    attention_heads.append(head_output)\n\n  concatenated = np.concatenate(attention_heads, axis=-1) # Shape: (N, num_heads * d_v)\n  output = np.dot(concatenated, W_O) # Shape: (N, d_model)\n\n  return output\n\n\n\nPositional Encoding\n\\[\n\\begin{aligned}\nPE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#applications-and-insights",
    "href": "pages/attention-is-all-you-need/index.html#applications-and-insights",
    "title": "Transformers: Attention Is All You Need",
    "section": "Applications and Insights",
    "text": "Applications and Insights\n\nWhere and how is this concept applied in the real world?\nWhat problems does it solve, and what value does it provide?\nWhat are some examples or use cases where this has been impactful?\nWhat are the broader implications or insights gained from this work?\nHow does this contribute to advancements in the field or industry?"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#conclusion",
    "href": "pages/attention-is-all-you-need/index.html#conclusion",
    "title": "Transformers: Attention Is All You Need",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat are the key takeaways or lessons from this guide?\nWhy is this concept significant in the broader context of the field?\nWhat questions remain unanswered or open for further exploration?\nWhat resources or next steps can help deepen understanding?\nHow can this knowledge be applied or expanded upon in practice?"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Title",
    "section": "",
    "text": "Paper Name by Author Name et al., year."
  },
  {
    "objectID": "template.html#introduction",
    "href": "template.html#introduction",
    "title": "Title",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the topic of this guide, and why is it important?\nWhat problem does this concept or paper aim to solve?\nWhat are the main contributions or breakthroughs introduced?\nHow does this fit into the broader context of the field?\nWho would benefit from understanding this guide, and what will they learn?"
  },
  {
    "objectID": "template.html#background-and-motivation",
    "href": "template.html#background-and-motivation",
    "title": "Title",
    "section": "Background and Motivation",
    "text": "Background and Motivation\n\nWhat challenges or problems existed before this work?\nWhat inspired this idea, concept, or solution?\nHow does this approach differ from previous methods?\nWhat are the key assumptions or principles that underpin this work?\nWhy was solving this problem important or necessary?"
  },
  {
    "objectID": "template.html#methods",
    "href": "template.html#methods",
    "title": "Title",
    "section": "Methods",
    "text": "Methods\nGeneral Questions: * What are the foundational ideas or building blocks? * How does this concept or mechanism work step-by-step? * What are the inputs, processes, and outputs involved? * What are the strengths and limitations of this approach? * What are the key equations, algorithms, or methods associated with it?"
  },
  {
    "objectID": "template.html#visualization-questions",
    "href": "template.html#visualization-questions",
    "title": "Title",
    "section": "Visualization Questions:",
    "text": "Visualization Questions:\n\nCan this idea or process be visualized? If so, how?\nWhat diagrams or graphs can help illustrate the key points?\nHow can we interpret the behavior or results visually?"
  },
  {
    "objectID": "template.html#code-questions",
    "href": "template.html#code-questions",
    "title": "Title",
    "section": "Code Questions:",
    "text": "Code Questions:\n\nHow can this concept be implemented in code?\nWhat are the key components or parameters of the implementation?\nWhat are some practical examples or demonstrations of this idea?\nHow does the code reflect the theory or process behind the concept?"
  },
  {
    "objectID": "template.html#application-and-insights",
    "href": "template.html#application-and-insights",
    "title": "Title",
    "section": "Application and Insights",
    "text": "Application and Insights\n\nWhere and how is this concept applied in the real world?\nWhat problems does it solve, and what value does it provide?\nWhat are some examples or use cases where this has been impactful?\nWhat are the broader implications or insights gained from this work?\nHow does this contribute to advancements in the field or industry?"
  },
  {
    "objectID": "template.html#conclusion",
    "href": "template.html#conclusion",
    "title": "Title",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat are the key takeaways or lessons from this guide?\nWhy is this concept significant in the broader context of the field?\nWhat questions remain unanswered or open for further exploration?\nWhat resources or next steps can help deepen understanding?\nHow can this knowledge be applied or expanded upon in practice?"
  }
]