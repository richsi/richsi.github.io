[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Richard’s Page",
    "section": "",
    "text": "Test page\n\n\n\n\n\n\nExample\n\n\nTest\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\nrichsi\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers: Attention Is All You Need\n\n\n\n\n\n\nAttention\n\n\nPaper\n\n\nTransformer\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\nrichsi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I’m Richard"
  },
  {
    "objectID": "pages/test_page/index.html",
    "href": "pages/test_page/index.html",
    "title": "Test page",
    "section": "",
    "text": "Euler’s formula is \\(e^{i\\pi} + 1 = 0\\).\n\\[\n\\int_a^b f(x) dx = F(b) - F(a)\n\\]"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html",
    "href": "pages/attention-is-all-you-need/index.html",
    "title": "Transformers: Attention Is All You Need",
    "section": "",
    "text": "“Attention Is All You Need” by Ashish Vaswani et al., 2017.\nImporting packages\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#introduction",
    "href": "pages/attention-is-all-you-need/index.html#introduction",
    "title": "Transformers: Attention Is All You Need",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the topic of this guide, and why is it important?\n\nThe transformer is an attention-based network architecture that learns context and meaning by tracking relationships in sequential data like words in a sentence.\n\nWhat problem does this concept or paper aim to solve?\n\nSequence modeling and transduction problems such as machine translation have previously been solved by recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). However, due to sequential dependency, the inherent nature of RNNs prevents its training from being parallelized.\n\nWhat are the main contributions or breakthroughs introduced?\n\nTransformers show significant improvements in both computational efficiency as well as model performance.\n\nHow does this fit into the broader context of the field?\n\nThis paper laid the groundwork for state-of-the-art models such as BERT and GPT which has revolutionized the natural language processing (NLP) field. Beyong NLP, transformers have successfully adapted to other machine learning domains like computer vision and reinforcement learning."
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#methods",
    "href": "pages/attention-is-all-you-need/index.html#methods",
    "title": "Transformers: Attention Is All You Need",
    "section": "Methods",
    "text": "Methods\n\nEncoder Decoder Stacks\nEncoder:\nDecoder:\n\n\nScaled Dot-Product Attention\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V\n\\]\nScaled Dot-Product Attention takes in three inputs: query \\(\\bf{Q}\\), key \\(\\bf{K}\\), and value \\(\\bf{V}\\), where \\(X\\) is the input and \\(W^Q\\), \\(W^K\\), \\(W^V\\) are learnable weight matrices specific to queries, keys, and values.\n\\[\n\\begin{aligned}\nQ = XW^Q\\\\\nK = XW^K\\\\\nV = XW^V\\\\\n\\end{aligned}\n\\]\n\n\\(\\bf{X}\\) represents the input embeddings to the transformer layer. For the first layer, \\(X\\) = Word Embedding + Positional Encoding. Word embedding is a vector that represents the semantic meaning of a word (or token). Positional encoding is a vector that contains informations about a token’s position in a sequence by mapping that information to a latent space.\n\\(\\bf{Q}\\) represents the current token’s representation to “query” information from other tokens. It’s derived from the combined word and positional information.\n\\(\\bf{K}\\) represents all tokens in the space and helps determine the relevance of each token to the query token. Like \\(Q\\), it’s based on the same input embeddings (word + positional) but transformed different via \\(W^K\\).\n\\(\\bf{V}\\) represents the information associated with each token. It is the actual data that gets weighted and aggregated based on attention scores.\n\nThe dimensions of \\(Q\\), \\(K\\), and \\(V\\) are determined by the model’s latent space dimensionality (\\(d_{model}\\)) and the number of attention heads (\\(h\\)). In this example, we initialize the latent space dimension to be 512 and the number of attention heads to be 8.\n\ndef scaled_dot_product_attention(Q, K, V):\n  \"\"\" \n  Compute scaled dot-product attention\n\n  Args:\n  Q: Query matrix, shape (N, d_k)\n  K: Key matrix, shape (N, d_k)\n  V: Value matrix, shape (N, d_v)\n\n  Returns:\n  output: Weighted sum of values after applying attention, shape (N, d_v)\n  attention_weights: shape (N, N)\n  \"\"\"\n  d_k = Q.shape[1] # Dimensionality of keys/queries\n  # Computing scaled attention scores\n  scores = np.dot(Q, K.T) / np.sqrt(d_k) # Shape: (N,d_v) @ (d_v,N) -&gt; (N, N)\n  \n  # Apply softmax to get attention weights\n  attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1) # axis=1 for column wise addition\n\n  # Computing weighted sum of values\n  output = np.dot(attention_weights, V) # Shape: (N,N) @ (N,d_v) -&gt; (N,d_v)\n\n  return output, attention_weights\n\n\n\nMulti-Head Attention\n\n\nPositional Encoding\n\\[\n\\begin{aligned}\nPE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \\\\\nPE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#applications-and-insights",
    "href": "pages/attention-is-all-you-need/index.html#applications-and-insights",
    "title": "Transformers: Attention Is All You Need",
    "section": "Applications and Insights",
    "text": "Applications and Insights\n\nWhere and how is this concept applied in the real world?\nWhat problems does it solve, and what value does it provide?\nWhat are some examples or use cases where this has been impactful?\nWhat are the broader implications or insights gained from this work?\nHow does this contribute to advancements in the field or industry?"
  },
  {
    "objectID": "pages/attention-is-all-you-need/index.html#conclusion",
    "href": "pages/attention-is-all-you-need/index.html#conclusion",
    "title": "Transformers: Attention Is All You Need",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat are the key takeaways or lessons from this guide?\nWhy is this concept significant in the broader context of the field?\nWhat questions remain unanswered or open for further exploration?\nWhat resources or next steps can help deepen understanding?\nHow can this knowledge be applied or expanded upon in practice?"
  },
  {
    "objectID": "template.html",
    "href": "template.html",
    "title": "Title",
    "section": "",
    "text": "Paper Name by Author Name et al., year."
  },
  {
    "objectID": "template.html#introduction",
    "href": "template.html#introduction",
    "title": "Title",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is the topic of this guide, and why is it important?\nWhat problem does this concept or paper aim to solve?\nWhat are the main contributions or breakthroughs introduced?\nHow does this fit into the broader context of the field?\nWho would benefit from understanding this guide, and what will they learn?"
  },
  {
    "objectID": "template.html#background-and-motivation",
    "href": "template.html#background-and-motivation",
    "title": "Title",
    "section": "Background and Motivation",
    "text": "Background and Motivation\n\nWhat challenges or problems existed before this work?\nWhat inspired this idea, concept, or solution?\nHow does this approach differ from previous methods?\nWhat are the key assumptions or principles that underpin this work?\nWhy was solving this problem important or necessary?"
  },
  {
    "objectID": "template.html#methods",
    "href": "template.html#methods",
    "title": "Title",
    "section": "Methods",
    "text": "Methods\nGeneral Questions: * What are the foundational ideas or building blocks? * How does this concept or mechanism work step-by-step? * What are the inputs, processes, and outputs involved? * What are the strengths and limitations of this approach? * What are the key equations, algorithms, or methods associated with it?"
  },
  {
    "objectID": "template.html#visualization-questions",
    "href": "template.html#visualization-questions",
    "title": "Title",
    "section": "Visualization Questions:",
    "text": "Visualization Questions:\n\nCan this idea or process be visualized? If so, how?\nWhat diagrams or graphs can help illustrate the key points?\nHow can we interpret the behavior or results visually?"
  },
  {
    "objectID": "template.html#code-questions",
    "href": "template.html#code-questions",
    "title": "Title",
    "section": "Code Questions:",
    "text": "Code Questions:\n\nHow can this concept be implemented in code?\nWhat are the key components or parameters of the implementation?\nWhat are some practical examples or demonstrations of this idea?\nHow does the code reflect the theory or process behind the concept?"
  },
  {
    "objectID": "template.html#application-and-insights",
    "href": "template.html#application-and-insights",
    "title": "Title",
    "section": "Application and Insights",
    "text": "Application and Insights\n\nWhere and how is this concept applied in the real world?\nWhat problems does it solve, and what value does it provide?\nWhat are some examples or use cases where this has been impactful?\nWhat are the broader implications or insights gained from this work?\nHow does this contribute to advancements in the field or industry?"
  },
  {
    "objectID": "template.html#conclusion",
    "href": "template.html#conclusion",
    "title": "Title",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat are the key takeaways or lessons from this guide?\nWhy is this concept significant in the broader context of the field?\nWhat questions remain unanswered or open for further exploration?\nWhat resources or next steps can help deepen understanding?\nHow can this knowledge be applied or expanded upon in practice?"
  }
]